{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "sys.path.append('../')\n",
    "from util.visualizer import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_symG(data, ndf=64, use_dropout=False, n_blocks=9, no_bias=True, fix_gamma=True, eps=1e-5 + 1e-12):\n",
    "    \n",
    "    BatchNorm = mx.sym.BatchNorm\n",
    "    \n",
    "    ## Convolution BatchNorm RELU layer\n",
    "    c1 = mx.sym.Convolution(data, name='c1', kernel=(7, 7), pad=(3, 3), stride=(1, 1), num_filter=ndf, no_bias=no_bias)\n",
    "    cbn1 = BatchNorm(c1, name='cbn1', fix_gamma=fix_gamma, eps=eps)\n",
    "    cact1 = mx.sym.Activation(cbn1, name='cact1', act_type='relu')\n",
    "    \n",
    "    c2 = mx.sym.Convolution(cact1, name='c2', kernel=(3, 3), pad=(1, 1), stride=(2, 2), num_filter=ndf*2, no_bias=no_bias)\n",
    "    cbn2 = BatchNorm(c2, name='cbn2', fix_gamma=fix_gamma, eps=eps)\n",
    "    cact2 = mx.sym.Activation(cbn2, name='cact2', act_type='relu')\n",
    "    \n",
    "    c3 = mx.sym.Convolution(cact2, name='c3', kernel=(3, 3), pad=(1, 1), stride=(2, 2), num_filter=ndf*4, no_bias=no_bias)\n",
    "    cbn3 = BatchNorm(c3, name='cbn3', fix_gamma=fix_gamma, eps=eps)\n",
    "    cact3 = mx.sym.Activation(cbn3, name='cact3', act_type='relu')\n",
    "    \n",
    "    ## Resnet Block\n",
    "    reslayer_out = cact3\n",
    "    for i in range(n_blocks):\n",
    "        reslayer = mx.sym.Convolution(reslayer_out, name='resc1_%d'%i, kernel=(3, 3), pad=(1, 1), num_filter=ndf*4, no_bias=no_bias)\n",
    "        reslayer = BatchNorm(reslayer, name='resbn1_%d'%i, fix_gamma=fix_gamma, eps=eps)\n",
    "        reslayer = mx.sym.Activation(reslayer, name='resact%d'%i, act_type='relu')\n",
    "        reslayer = mx.sym.Convolution(reslayer, name='resc2_%d'%i, kernel=(3, 3), pad=(1, 1), num_filter=ndf*4, no_bias=no_bias)\n",
    "        reslayer = BatchNorm(reslayer, name='resbn2_%d'%i, fix_gamma=fix_gamma, eps=eps)\n",
    "        reslayer_out = reslayer_out + reslayer\n",
    "    \n",
    "    ## Deconvolution Layer\n",
    "    d1 = mx.sym.Deconvolution(reslayer_out, name='d1', kernel=(3, 3), pad=(1, 1), stride=(2, 2), adj=(1, 1), num_filter=ndf*2, no_bias=no_bias)\n",
    "    dbn1 = BatchNorm(d1, name='dbn1', fix_gamma=fix_gamma, eps=eps)\n",
    "    dact1 = mx.sym.Activation(dbn1, name='dact1', act_type='relu')\n",
    "    \n",
    "    d2 = mx.sym.Deconvolution(dact1, name='d2', kernel=(3, 3), pad=(1, 1), stride=(2, 2), adj=(1, 1), num_filter=ndf, no_bias=no_bias)\n",
    "    dbn2 = BatchNorm(d2, name='dbn2', fix_gamma=fix_gamma, eps=eps)\n",
    "    dact2 = mx.sym.Activation(dbn2, name='dact2', act_type='relu')\n",
    "    \n",
    "    c4 = mx.sym.Convolution(dact2, name='c4', kernel=(7, 7), pad=(3, 3), stride=(1, 1), num_filter=3, no_bias=no_bias)\n",
    "    gout = mx.sym.Activation(c4, name='gout', act_type='tanh')\n",
    "    \n",
    "    return gout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_symD(data, label, ndf=64, no_bias=True, fix_gamma=True, eps=1e-5 + 1e-12):\n",
    "    BatchNorm = mx.sym.BatchNorm\n",
    "    d1 = mx.sym.Convolution(data, name='d1', kernel=(4, 4), stride=(\n",
    "        2, 2), pad=(2, 2), num_filter=ndf, no_bias=no_bias)\n",
    "    dact1 = mx.sym.LeakyReLU(d1, name='dact1', act_type='leaky', slope=0.2)\n",
    "\n",
    "    d2 = mx.sym.Convolution(dact1, name='d2', kernel=(4, 4), stride=(\n",
    "        2, 2), pad=(2, 2), num_filter=ndf * 2, no_bias=no_bias)\n",
    "    dbn2 = BatchNorm(d2, name='dbn2', fix_gamma=fix_gamma, eps=eps)\n",
    "    dact2 = mx.sym.LeakyReLU(dbn2, name='dact2', act_type='leaky', slope=0.2)\n",
    "\n",
    "    d3 = mx.sym.Convolution(dact2, name='d3', kernel=(4, 4), stride=(\n",
    "        2, 2), pad=(2, 2), num_filter=ndf * 4, no_bias=no_bias)\n",
    "    dbn3 = BatchNorm(d3, name='dbn3', fix_gamma=fix_gamma, eps=eps)\n",
    "    dact3 = mx.sym.LeakyReLU(dbn3, name='dact3', act_type='leaky', slope=0.2)\n",
    "\n",
    "    d4 = mx.sym.Convolution(dact3, name='d4', kernel=(4, 4), stride=(\n",
    "        1, 1), pad=(2, 2), num_filter=ndf * 8, no_bias=no_bias)\n",
    "    dbn4 = BatchNorm(d4, name='dbn4', fix_gamma=fix_gamma, eps=eps)\n",
    "    dact4 = mx.sym.LeakyReLU(dbn4, name='dact4', act_type='leaky', slope=0.2)\n",
    "\n",
    "    d5 = mx.sym.Convolution(dact4, name='d5', kernel=(4, 4), stride=(\n",
    "        1, 1), pad=(2, 2), num_filter=1, no_bias=no_bias)\n",
    "    \n",
    "    # fc = mx.sym.FullyConnected(data = d5, num_hidden=1, name='output')\n",
    "    \n",
    "    mseloss_ = mx.sym.mean(mx.sym.square(d5 - label), name='mean')\n",
    "    mseloss = mx.sym.MakeLoss(data=mseloss_, name='mean_square_loss')\n",
    "    \n",
    "    return mseloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_cycleGAN():\n",
    "    # Generator\n",
    "    dataA = mx.sym.Variable('dataA')\n",
    "    dataB = mx.sym.Variable('dataB')\n",
    "    symG_A = make_symG(dataA, 32)\n",
    "    symG_B = make_symG(dataB, 32)\n",
    "    \n",
    "    # Discriminator\n",
    "    dataC = mx.sym.Variable('dataC')\n",
    "    dataD = mx.sym.Variable('dataD')\n",
    "    labelC = mx.sym.Variable('labelC')\n",
    "    labelD = mx.sym.Variable('labelD')\n",
    "    symD_A = make_symD(dataC, labelC)\n",
    "    symD_B = make_symD(dataD, labelD)\n",
    "    \n",
    "    return symG_A, symG_B, symD_A, symD_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndf = 32\n",
    "ngf = 32\n",
    "nc = 3\n",
    "batch_size = 1\n",
    "Z = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "ctx = mx.gpu(0)\n",
    "check_point = False\n",
    "label = mx.nd.zeros((batch_size, 1, 35, 35), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "symG_A, symG_B, symD_A, symD_B = make_cycleGAN()\n",
    "# Generator A\n",
    "modG_A = mx.mod.Module(symbol=symG_A, data_names=(\n",
    "    'dataA',), label_names=None, context=ctx)\n",
    "modG_A.bind(data_shapes=[('dataA', (batch_size, 3, 256, 256))],\n",
    "           inputs_need_grad=True)\n",
    "modG_A.init_params(initializer=mx.init.Normal(0.02))\n",
    "modG_A.init_optimizer(\n",
    "    optimizer='adam',\n",
    "    optimizer_params={\n",
    "        'learning_rate': lr,\n",
    "        'wd': 0.,\n",
    "        'beta1': beta1,\n",
    "    })\n",
    "\n",
    "# Generator B\n",
    "modG_B = mx.mod.Module(symbol=symG_B, data_names=(\n",
    "    'dataB',), label_names=None, context=ctx)\n",
    "modG_B.bind(data_shapes=[('dataB', (batch_size, 3, 256, 256))],\n",
    "           inputs_need_grad=True)\n",
    "modG_B.init_params(initializer=mx.init.Normal(0.02))\n",
    "modG_B.init_optimizer(\n",
    "    optimizer='adam',\n",
    "    optimizer_params={\n",
    "        'learning_rate': lr,\n",
    "        'wd': 0.,\n",
    "        'beta1': beta1,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator A\n",
    "modD_A = mx.mod.Module(symbol=symD_A, data_names=(\n",
    "    'dataC',), label_names=('labelC',), context=ctx)\n",
    "modD_A.bind(data_shapes=[('dataC', (batch_size, 3, 256, 256))], \n",
    "            label_shapes=[('labelC', (batch_size, 1, 35, 35))],\n",
    "            inputs_need_grad=True)\n",
    "modD_A.init_params(initializer=mx.init.Normal(0.02))\n",
    "modD_A.init_optimizer(\n",
    "    optimizer='adam',\n",
    "    optimizer_params={\n",
    "        'learning_rate': lr,\n",
    "        'wd': 0.,\n",
    "        'beta1': beta1,\n",
    "    })\n",
    "\n",
    "# Discriminator B\n",
    "modD_B = mx.mod.Module(symbol=symD_B, data_names=(\n",
    "    'dataD',), label_names=('labelD',), context=ctx)\n",
    "modD_B.bind(data_shapes=[('dataD', (batch_size, 3, 256, 256))], \n",
    "            label_shapes=[('labelD', (batch_size, 1, 35, 35))],\n",
    "            inputs_need_grad=True)\n",
    "modD_B.init_params(initializer=mx.init.Normal(0.02))\n",
    "modD_B.init_optimizer(\n",
    "    optimizer='adam',\n",
    "    optimizer_params={\n",
    "        'learning_rate': lr,\n",
    "        'wd': 0.,\n",
    "        'beta1': beta1,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Image Iterator\n",
    "class ImagenetIter(mx.io.DataIter):\n",
    "\n",
    "    def __init__(self, path, batch_size, data_shape):\n",
    "        self.internal = mx.image.ImageIter(\n",
    "            imglist=[[1, img] for img in path],\n",
    "            data_shape=data_shape,\n",
    "            batch_size=batch_size,\n",
    "            path_root='./',\n",
    "            )\n",
    "        self.provide_data = [('data', (batch_size,) + data_shape)]\n",
    "        self.provide_label = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.internal.reset()\n",
    "\n",
    "    def iter_next(self):\n",
    "        return self.internal.iter_next()\n",
    "\n",
    "    def getdata(self):\n",
    "        data = self.internal.next().data[0]\n",
    "        data = data * (2.0 / 255.0)\n",
    "        data -= 1\n",
    "        return [data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image list...\n",
      "loading image list...\n"
     ]
    }
   ],
   "source": [
    "## load train data to iterator\n",
    "horses = glob.glob('../../datasets/vangogh2photo/trainA/*.jpg')\n",
    "zebras = glob.glob('../../datasets/vangogh2photo/trainB/*.jpg')\n",
    "horses_iter = ImagenetIter(horses, batch_size, (3, 256, 256))\n",
    "zebras_iter = ImagenetIter(zebras, batch_size, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image list...\n",
      "loading image list...\n"
     ]
    }
   ],
   "source": [
    "## load test data to iterator\n",
    "horses = glob.glob('../../datasets/vangogh2photo/testA/*.jpg')\n",
    "zebras = glob.glob('../../datasets/vangogh2photo/testB/*.jpg')\n",
    "horses_test_iter = ImagenetIter(horses, batch_size, (3, 256, 256))\n",
    "zebras_test_iter = ImagenetIter(zebras, batch_size, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputA = horses_iter.getdata()\n",
    "inputB = zebras_iter.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "horses_iter.reset()\n",
    "zebras_iter.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_l1grad(cycle, real, lamb=150, sigma=100):\n",
    "    l1_g = (cycle.asnumpy() - real.asnumpy())\n",
    "    l1_loss = np.mean(np.abs(l1_g))\n",
    "    l1_g =  l1_g * sigma * sigma     \n",
    "    l1_g[l1_g > 1] = 1\n",
    "    l1_g[l1_g < -1] = -1\n",
    "    grad = mx.ndarray.array(l1_g * lamb *(1.0/(256.0*256.0*3)), ctx=ctx)\n",
    "    return l1_loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_generator(inputA, inputB):\n",
    "    # calculate loss for inputA\n",
    "    modG_A.forward(mx.io.DataBatch(data=inputA, label=None), is_train=True)\n",
    "    fakeB = modG_A.get_outputs()\n",
    "    modG_B.forward(mx.io.DataBatch(data=fakeB, label=None), is_train=True)\n",
    "    cycleA = modG_B.get_outputs()\n",
    "\n",
    "    # backward for cycle L1 loss for inputA and cycleA\n",
    "    l1lossA, grad = get_l1grad(cycleA[0], inputA[0])\n",
    "    modG_B.backward([grad])\n",
    "\n",
    "    # backward for GAN loss\n",
    "    label[:] = 1\n",
    "    modD_B.forward(mx.io.DataBatch(data=fakeB, label=[label]), is_train=True)\n",
    "    modD_B.backward()\n",
    "\n",
    "    modG_A.backward([modG_B.get_input_grads()[0] + modD_B.get_input_grads()[0]])\n",
    "\n",
    "    # save gradients for future update\n",
    "    gradG_A = [[grad.copyto(grad.context) for grad in grads] for grads in modG_A._exec_group.grad_arrays]\n",
    "    gradG_B = [[grad.copyto(grad.context) for grad in grads] for grads in modG_B._exec_group.grad_arrays]\n",
    "\n",
    "    ## Calculate loss for inputB\n",
    "    modG_B.forward(mx.io.DataBatch(data=inputB, label=None), is_train=True)\n",
    "    fakeA = modG_B.get_outputs()\n",
    "    modG_A.forward(mx.io.DataBatch(data=fakeA, label=None), is_train=True)\n",
    "    cycleB = modG_A.get_outputs()\n",
    "\n",
    "    # backward for cycle L1 loss for inputB and cycleB\n",
    "    l1lossB, grad = get_l1grad(cycleB[0], inputB[0])\n",
    "    modG_A.backward([grad])\n",
    "\n",
    "    # backward for GAN loss\n",
    "    label[:] = 1\n",
    "    modD_A.forward(mx.io.DataBatch(data=fakeA, label=[label]), is_train=True)\n",
    "    modD_A.backward()\n",
    "\n",
    "    modG_B.backward([modG_A.get_input_grads()[0] + modD_A.get_input_grads()[0]])\n",
    "\n",
    "    # update Generator A and Generator B\n",
    "    for gradsr, gradsf in zip(modG_A._exec_group.grad_arrays, gradG_A):\n",
    "        for gradr, gradf in zip(gradsr, gradsf):\n",
    "            gradr += gradf\n",
    "    modG_A.update()\n",
    "\n",
    "    for gradsr, gradsf in zip(modG_B._exec_group.grad_arrays, gradG_B):\n",
    "        for gradr, gradf in zip(gradsr, gradsf):\n",
    "            gradr += gradf\n",
    "    modG_B.update()\n",
    "    \n",
    "    return l1lossA, l1lossB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_discriminator(modD, real, fake):\n",
    "    # train with real data\n",
    "    label[:] = 1\n",
    "    modD.forward(mx.io.DataBatch(data=real, label=[label]), is_train=True)\n",
    "    modD.backward()\n",
    "    # save gradient for future use\n",
    "    gradD = [[grad.copyto(grad.context) for grad in grads] for grads in modD._exec_group.grad_arrays]\n",
    "    \n",
    "    # loss of discriminator\n",
    "    loss = modD.get_outputs()[0].asnumpy()[0]\n",
    "    \n",
    "    # train with fake data\n",
    "    label[:] = 0\n",
    "    modD.forward(mx.io.DataBatch(data=fake, label=[label]), is_train=True)\n",
    "    modD.backward()\n",
    "    loss += modD.get_outputs()[0].asnumpy()[0]\n",
    "    for gradsr, gradsf in zip(modD._exec_group.grad_arrays, gradD):\n",
    "        for gradr, gradf in zip(gradsr, gradsf):\n",
    "            gradr += gradf\n",
    "    modD.update()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testA = horses_test_iter.getdata()\n",
    "testB = zebras_test_iter.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirname = './cycle_output_new'\n",
    "if not os.path.exists(dirname):\n",
    "    os.makedirs(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../util/visualizer.py:17: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  buff = np.zeros((n*X.shape[1], n*X.shape[2], X.shape[3]), dtype=np.uint8)\n",
      "/usr/lib/python2.7/site-packages/mxnet-0.9.4-py2.7.egg/mxnet/module/executor_group.py:371: UserWarning: Calling forward the second time after forward(is_train=True) without calling backward first. Is this intended?\n",
      "  exec_.forward(is_train=is_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0_0 lossD_A: 3.96132 lossD_B: 2.77213 l1loss_a:0.665736 l1loss_b:0.516695\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    horses_iter.reset()\n",
    "    zebras_iter.reset()\n",
    "    if (i % 1 == 0):\n",
    "        inputA = horses_iter.getdata()\n",
    "        inputB = zebras_iter.getdata()\n",
    "        # visualize A-B-A\n",
    "        modG_A.forward(mx.io.DataBatch(data=inputA, label=None), is_train=True)\n",
    "        fakeB = modG_A.get_outputs()\n",
    "        modG_B.forward(mx.io.DataBatch(data=fakeB, label=None), is_train=True)\n",
    "        cycleA = modG_B.get_outputs()\n",
    "        A_B_A = np.concatenate((inputA[0].asnumpy(), fakeB[0].asnumpy(), cycleA[0].asnumpy()))\n",
    "        visual(os.path.join(dirname, 'A_B_A'+str(i)+'.jpg'), A_B_A)\n",
    "        \n",
    "        # visualize B-A-B\n",
    "        modG_B.forward(mx.io.DataBatch(data=inputB, label=None), is_train=True)\n",
    "        fakeA = modG_B.get_outputs()\n",
    "        modG_A.forward(mx.io.DataBatch(data=fakeA, label=None), is_train=True)\n",
    "        cycleB = modG_A.get_outputs()\n",
    "        B_A_B = np.concatenate((inputB[0].asnumpy(), fakeA[0].asnumpy(), cycleB[0].asnumpy()))\n",
    "        visual(os.path.join(dirname, 'B_A_B'+str(i)+'.jpg'), B_A_B)\n",
    "    \n",
    "    # reset image iterator\n",
    "    horses_iter.reset()\n",
    "    zebras_iter.reset()\n",
    "    for j in range(399):\n",
    "        inputA = horses_iter.getdata()\n",
    "        inputB = zebras_iter.getdata()\n",
    "        l1lossA, l1lossB = update_generator(inputA, inputB)\n",
    "        modG_A.forward(mx.io.DataBatch(data=inputA, label=None), is_train=True)\n",
    "        fakeB = modG_A.get_outputs()\n",
    "        modG_B.forward(mx.io.DataBatch(data=inputB, label=None), is_train=True)\n",
    "        fakeA = modG_B.get_outputs()\n",
    "        lossD_A = update_discriminator(modD_A, inputA, fakeA)\n",
    "        lossD_B = update_discriminator(modD_B, inputB, fakeB)\n",
    "        if (j % 200 == 0):\n",
    "            print 'epoch: ' + str(i) + '_' + str(j) + ' lossD_A: ' + str(lossD_A) + ' lossD_B: ' + str(lossD_B) + ' l1loss_a:' + str(l1lossA) + ' l1loss_b:' + str(l1lossB)\n",
    "            visual(os.path.join(dirname, 'fakeA'+str(i)+'_'+str(j/100)+'.jpg'), fakeA[0].asnumpy())\n",
    "            visual(os.path.join(dirname, 'fakeB'+str(i)+'_'+str(j/100)+'.jpg'), fakeB[0].asnumpy())\n",
    "            modG_A.forward(mx.io.DataBatch(data=testA, label=None), is_train=True)\n",
    "            modG_B.forward(mx.io.DataBatch(data=testB, label=None), is_train=True)\n",
    "            visual(os.path.join(dirname, 'testB'+str(i)+'_'+str(j/100)+'.jpg'), modG_A.get_outputs()[0].asnumpy())\n",
    "            visual(os.path.join(dirname, 'testA'+str(i)+'_'+str(j/100)+'.jpg'), modG_B.get_outputs()[0].asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below are used for test function or so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = np.concatenate((cycleA[0].asnumpy(), inputA[0].asnumpy(), fakeB[0].asnumpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visual(os.path.join(dirname, 'aaa.jpg'), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputA = horses_iter.getdata()\n",
    "inputB = zebras_iter.getdata()\n",
    "l1lossA, l1lossB = update_generator(inputA, inputB)\n",
    "modG_A.forward(mx.io.DataBatch(data=inputA, label=None), is_train=True)\n",
    "fakeB = modG_A.get_outputs()\n",
    "modG_B.forward(mx.io.DataBatch(data=inputB, label=None), is_train=True)\n",
    "fakeA = modG_B.get_outputs()\n",
    "lossA = update_discriminator(modD_A, inputA, fakeA)\n",
    "lossB = update_discriminator(modD_B, inputB, fakeB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modG_A.forward(mx.io.DataBatch(data=inputA, label=None), is_train=True)\n",
    "fakeB = modG_A.get_outputs()\n",
    "modG_B.forward(mx.io.DataBatch(data=fakeB, label=None), is_train=True)\n",
    "cycleA = modG_B.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modG_B.forward(mx.io.DataBatch(data=inputB, label=None), is_train=True)\n",
    "fakeA = modG_B.get_outputs()\n",
    "modG_A.forward(mx.io.DataBatch(data=fakeA, label=None), is_train=True)\n",
    "cycleB = modG_A.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visual(os.path.join(dirname, 'B_2.jpg'), cycleB[0].asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label[:] = 0\n",
    "modD_B.forward(mx.io.DataBatch(data=fakeB, label=[label]), is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = symD_B.get_internals()['output_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator B\n",
    "modD_O = mx.mod.Module(symbol=output, data_names=(\n",
    "    'dataD',), label_names=None, context=ctx)\n",
    "modD_O.bind(data_shapes=[('dataD', (batch_size, 3, 256, 256))], \n",
    "            inputs_need_grad=True)\n",
    "modD_O.init_params(initializer=mx.init.Normal(0.02))\n",
    "modD_O.init_optimizer(\n",
    "    optimizer='adam',\n",
    "    optimizer_params={\n",
    "        'learning_rate': lr,\n",
    "        'wd': 0.,\n",
    "        'beta1': beta1,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modD_O.forward(mx.io.DataBatch(data=fakeB, label=[label]), is_train=True)\n",
    "c = modD_O.get_outputs()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of aux_states do not match number of arguments",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-2f7c729e8cbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/mxnet-0.9.4-py2.7.egg/mxnet/symbol.pyc\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, ctx, args, args_grad, grad_req, aux_states, group2ctx, shared_exec)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0maux_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         aux_args_handle, aux_states = self._get_ndarray_inputs(\n\u001b[0;32m--> 952\u001b[0;31m             'aux_states', aux_states, self.list_auxiliary_states(), False)\n\u001b[0m\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# setup requirements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/mxnet-0.9.4-py2.7.egg/mxnet/symbol.pyc\u001b[0m in \u001b[0;36m_get_ndarray_inputs\u001b[0;34m(arg_key, args, arg_names, allow_missing)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Length of %s do not match number of arguments'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0marg_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnarr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of aux_states do not match number of arguments"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "output.bind(ctx=ctx, args=new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = dict({'dataD' : fakeB[0]})\n",
    "new.update(modD_B.get_params()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d1_weight': <NDArray 64x3x4x4 @cpu(0)>,\n",
       " 'd2_weight': <NDArray 128x64x4x4 @cpu(0)>,\n",
       " 'd3_weight': <NDArray 256x128x4x4 @cpu(0)>,\n",
       " 'd4_weight': <NDArray 512x256x4x4 @cpu(0)>,\n",
       " 'd5_weight': <NDArray 1x512x4x4 @cpu(0)>,\n",
       " 'dataD': <NDArray 9x3x256x256 @gpu(0)>,\n",
       " 'dbn2_beta': <NDArray 128 @cpu(0)>,\n",
       " 'dbn2_gamma': <NDArray 128 @cpu(0)>,\n",
       " 'dbn3_beta': <NDArray 256 @cpu(0)>,\n",
       " 'dbn3_gamma': <NDArray 256 @cpu(0)>,\n",
       " 'dbn4_beta': <NDArray 512 @cpu(0)>,\n",
       " 'dbn4_gamma': <NDArray 512 @cpu(0)>,\n",
       " 'output_bias': <NDArray 1 @cpu(0)>,\n",
       " 'output_weight': <NDArray 1x1225 @cpu(0)>}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}